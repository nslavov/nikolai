---
layout: post
author: nslavov
title: High-throughput omics
date:   2022-11-10
description: High-input, High-throughput, high-output science
---





<p class="intro"><span class="dropcap">R</span>esearch is increasingly driven by large-scale data. Yet, generating data at *high-throughput* sometimes carries negative connotations, as reflected in this [quote](https://www.nature.com/articles/nrm2320) from Sydney Brenner:</p>  


<!-- <blockquote>Low input, high throughput, no output science</blockquote> -->

## The bad
Indeed, high-throughput data collection have often been associated with poor experimental design and data quality, and this association has stuck. Sometimes, the problem is that treatment groups coincide with analysis batches, which makes it impossible to distinguish between biological effects and technological artifacts (batch effects). Another common problem is that high-throughput data often focussed on what can be measurements at high-throughput even if when its is not the most direct and useful measurement for answering scientific questions. A further problem is the fact that sometimes the number of assays makes it difficult to evaluate the quality of individual assays. A related problem is that optimizing for high-throughput may directly undermine important aspects of the data. For example, increasing the number of single cells analyzed by RNA-seq may decrease the copies of transcripts samples per single cells.



Despite these problems, the large volume of data generated by high-throughput omics may support the publication of the results in influential journals. Sometimes, the problems manifest in incorrect inferences that create confusion and take years to sort out. Such flawed inferences shape a negative connotation of high-throughput methods.                       



## The good
While the problems described above happen, they do not need to. They are not fundamentally inherent to every high-throughput measurement. For example, a high-throughput measurement is not necessarily indirect. Analysis batches can and often are randomized with respect to biological treatments. The accuracy and depth of individual assays can be [preserved](https://www.nature.com/articles/s41587-022-01389-w) while increasing the throughput of analysis. Furthermore, high-throughput methods may facilitate the development of standardize and systematic analytical workflow that substantially improves data quality.           


## What to do?
How to avoid the bad and take advantage of the good? I think the key is suggested by the Brenner quote. We should modify 'Low input' to 'High input', which hopefully modifies the rest of the quote to:


<!--  <blockquote>High input, High throughput, high output science</blockquote> -->

What I mean here by high input is *high intellectual input*. I mean starting with a solid experimental design and measurements that are worth doing, not only easy to do and high throughput.

I think high-throughput has much to offer if imbedded in well motivated research focussed on the scientific questions, not merely on maximizing data volumes.   


<br>

------
<!--
## Comments
Please leave comments as responses to the tweet below:


<blockquote class="twitter-tweet tw-align-center" ><p lang="en" dir="ltr" >Forthcoming changes in my life motivated me to reflect on my views and to share some thoughts.<br><br>The first installment is in this post, which carries personal reverberations.<a href="https://t.co/FkW373Tl3j">https://t.co/FkW373Tl3j</a></p>&mdash; Prof. Nikolai Slavov (@slavov_n) <a href="https://twitter.com/slavov_n/status/1555146276609540096?ref_src=twsrc%5Etfw">August 4, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
-->
