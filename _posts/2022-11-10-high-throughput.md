---
layout: post
author: nslavov
title: High-throughput omics
date:   2022-11-10
description: High-input, High-throughput, high-output science
---




<p class="intro"><span class="dropcap">R</span>esearch is increasingly driven by large-scale data. Yet, generating data at <i>high-throughput</i> sometimes carries negative connotations, as reflected in this <a href="https://www.nature.com/articles/nrm2320" target="_blanck">quote</a> from Sydney Brenner:</p>  


*"Low input, high throughput, no output science"*

## The bad
Indeed, high-throughput data collection has often been associated with poor experimental design and data quality. Sometimes, the problem is that treatment groups coincide with analysis batches, which makes it [impossible](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4516019/) to distinguish between biological effects and technological artifacts (batch effects). Another common problem is that high-throughput data often focus on what *can* be measured at high-throughput rather that what *should* be measured to answer scientific questions. Thus, high data volumes are accumulated even though the data are not the most direct and useful measurement for the investigated questions. A further problem is the fact that sometimes large numbers of assays make it difficult to evaluate the quality of individual assays. A related problem is that optimizing for high-throughput may directly undermine other important aspects of the data. For example, increasing the number of single cells analyzed by RNA-seq may decrease the copies of transcripts sampled per single cells.



Despite these problems, large volumes of data generated by high-throughput omics may support the publication of the results in influential journals. Sometimes, the above problems manifest in incorrect inferences that create confusion and take years to sort out. Such flawed inferences shape a negative connotation of high-throughput methods.                       



## The good
While the problems described above happen, they are not inevitable. They are not fundamentally inherent to every high-throughput measurement. For example, a high-throughput measurement is not necessarily indirect. Analysis batches can and often are randomized with respect to biological treatments. The accuracy and depth of single-cell analysis can be [preserved](https://www.nature.com/articles/s41587-022-01389-w) while increasing the throughput, i.e., number of singled cells analyzed per unit time and cost. Furthermore, high-throughput methods may facilitate the development of standardized and systematic analytical workflow that substantially improves data quality.           


## What to do?
How to avoid the bad and take advantage of the good? I think the key is suggested by the Brenner quote. We should modify 'Low input' to 'High input', which hopefully modifies the rest of the quote to:


*"High input, High throughput, high output science"*

What I mean here by high input is *high intellectual input*. I mean starting with a solid experimental design and measurements that are worth doing, not only easy to do and high throughput.

I think high-throughput has much to offer if imbedded in well motivated research focussed on the scientific questions, not merely on maximizing data volumes.   


<br>


------
<!--
## Comments
Please leave comments as responses to the tweet below:


<blockquote class="twitter-tweet tw-align-center" ><p lang="en" dir="ltr" >Forthcoming changes in my life motivated me to reflect on my views and to share some thoughts.<br><br>The first installment is in this post, which carries personal reverberations.<a href="https://t.co/FkW373Tl3j">https://t.co/FkW373Tl3j</a></p>&mdash; Prof. Nikolai Slavov (@slavov_n) <a href="https://twitter.com/slavov_n/status/1555146276609540096?ref_src=twsrc%5Etfw">August 4, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
-->
